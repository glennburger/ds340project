{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attack *Algorithm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## l2_attack.py -- attack a network optimizing for l_2 distance\n",
    "##\n",
    "## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n",
    "##\n",
    "## This program is licenced under the BSD 2-Clause licence,\n",
    "## contained in the LICENCE file in this directory.\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BINARY_SEARCH_STEPS = 9  # number of times to adjust the constant with binary search\n",
    "MAX_ITERATIONS = 10000   # number of iterations to perform gradient descent\n",
    "ABORT_EARLY = True       # if we stop improving, abort gradient descent early\n",
    "LEARNING_RATE = 1e-2     # larger values converge faster to less accurate results\n",
    "TARGETED = True          # should we target one specific class? or just be wrong?\n",
    "CONFIDENCE = 0           # how strong the adversarial example should be\n",
    "INITIAL_CONST = 1e-3     # the initial constant c to pick as a first guess\n",
    "\n",
    "### GLENN's changed\n",
    "###Assessing model holistically, assessing model's variables\n",
    "\n",
    "### Will be using for loop to see if these are indeed the best variables for the job\n",
    "\n",
    "class CarliniL2:\n",
    "    def __init__(self, sess, model, batch_size=1, confidence = CONFIDENCE,\n",
    "                 targeted = TARGETED, learning_rate = LEARNING_RATE,\n",
    "                 binary_search_steps = BINARY_SEARCH_STEPS, max_iterations = MAX_ITERATIONS,\n",
    "                 abort_early = ABORT_EARLY, \n",
    "                 initial_const = INITIAL_CONST,\n",
    "                 boxmin = -0.5, boxmax = 0.5):\n",
    "        \"\"\"\n",
    "        The L_2 optimized attack. \n",
    "\n",
    "        This attack is the most efficient and should be used as the primary \n",
    "        attack to evaluate potential defenses.\n",
    "\n",
    "        Returns adversarial examples for the supplied model.\n",
    "\n",
    "        confidence: Confidence of adversarial examples: higher produces examples\n",
    "          that are farther away, but more strongly classified as adversarial.\n",
    "        batch_size: Number of attacks to run simultaneously.\n",
    "        targeted: True if we should perform a targetted attack, False otherwise.\n",
    "        learning_rate: The learning rate for the attack algorithm. Smaller values\n",
    "          produce better results but are slower to converge.\n",
    "        binary_search_steps: The number of times we perform binary search to\n",
    "          find the optimal tradeoff-constant between distance and confidence. \n",
    "        max_iterations: The maximum number of iterations. Larger values are more\n",
    "          accurate; setting too small will require a large learning rate and will\n",
    "          produce poor results.\n",
    "        abort_early: If true, allows early aborts if gradient descent gets stuck.\n",
    "        initial_const: The initial tradeoff-constant to use to tune the relative\n",
    "          importance of distance and confidence. If binary_search_steps is large,\n",
    "          the initial constant is not important.\n",
    "        boxmin: Minimum pixel value (default -0.5).\n",
    "        boxmax: Maximum pixel value (default 0.5).\n",
    "        \"\"\"\n",
    "\n",
    "        image_size, num_channels, num_labels = model.image_size, model.num_channels, model.num_labels\n",
    "        self.sess = sess\n",
    "        self.TARGETED = targeted\n",
    "        self.LEARNING_RATE = learning_rate\n",
    "        self.MAX_ITERATIONS = max_iterations\n",
    "        self.BINARY_SEARCH_STEPS = binary_search_steps\n",
    "        self.ABORT_EARLY = abort_early\n",
    "        self.CONFIDENCE = confidence\n",
    "        self.initial_const = initial_const\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.repeat = binary_search_steps >= 10\n",
    "\n",
    "        self.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK = False\n",
    "\n",
    "        shape = (batch_size,image_size,image_size,num_channels)\n",
    "        \n",
    "        # the variable we're going to optimize over\n",
    "        modifier = tf.Variable(np.zeros(shape,dtype=np.float32))\n",
    "\n",
    "        # these are variables to be more efficient in sending data to tf\n",
    "        self.timg = tf.Variable(np.zeros(shape), dtype=tf.float32)\n",
    "        self.tlab = tf.Variable(np.zeros((batch_size,num_labels)), dtype=tf.float32)\n",
    "        self.const = tf.Variable(np.zeros(batch_size), dtype=tf.float32)\n",
    "\n",
    "        # and here's what we use to assign them\n",
    "        self.assign_timg = tf.placeholder(tf.float32, shape)\n",
    "        self.assign_tlab = tf.placeholder(tf.float32, (batch_size,num_labels))\n",
    "        self.assign_const = tf.placeholder(tf.float32, [batch_size])\n",
    "        \n",
    "        # the resulting image, tanh'd to keep bounded from boxmin to boxmax\n",
    "        self.boxmul = (boxmax - boxmin) / 2.\n",
    "        self.boxplus = (boxmin + boxmax) / 2.\n",
    "        self.newimg = tf.tanh(modifier + self.timg) * self.boxmul + self.boxplus\n",
    "        \n",
    "        # prediction BEFORE-SOFTMAX of the model\n",
    "        self.output = model.predict(self.newimg)\n",
    "        \n",
    "        # distance to the input data\n",
    "        self.l2dist = tf.reduce_sum(tf.square(self.newimg-(tf.tanh(self.timg) * self.boxmul + self.boxplus)),[1,2,3])\n",
    "        \n",
    "        # compute the probability of the label class versus the maximum other\n",
    "        real = tf.reduce_sum((self.tlab)*self.output,1)\n",
    "        other = tf.reduce_max((1-self.tlab)*self.output - (self.tlab*10000),1)\n",
    "\n",
    "        if self.TARGETED:\n",
    "            # if targetted, optimize for making the other class most likely\n",
    "            loss1 = tf.maximum(0.0, other-real+self.CONFIDENCE)\n",
    "        else:\n",
    "            # if untargeted, optimize for making this class least likely.\n",
    "            loss1 = tf.maximum(0.0, real-other+self.CONFIDENCE)\n",
    "\n",
    "        # sum up the losses\n",
    "        self.loss2 = tf.reduce_sum(self.l2dist)\n",
    "        self.loss1 = tf.reduce_sum(self.const*loss1)\n",
    "        self.loss = self.loss1+self.loss2\n",
    "        \n",
    "        # Setup the adam optimizer and keep track of variables we're creating\n",
    "        start_vars = set(x.name for x in tf.global_variables())\n",
    "        optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE)\n",
    "        self.train = optimizer.minimize(self.loss, var_list=[modifier])\n",
    "        end_vars = tf.global_variables()\n",
    "        new_vars = [x for x in end_vars if x.name not in start_vars]\n",
    "\n",
    "        # these are the variables to initialize when we run\n",
    "        self.setup = []\n",
    "        self.setup.append(self.timg.assign(self.assign_timg))\n",
    "        self.setup.append(self.tlab.assign(self.assign_tlab))\n",
    "        self.setup.append(self.const.assign(self.assign_const))\n",
    "        \n",
    "        self.init = tf.variables_initializer(var_list=[modifier]+new_vars)\n",
    "\n",
    "    def attack(self, imgs, targets):\n",
    "        \"\"\"\n",
    "        Perform the L_2 attack on the given images for the given targets.\n",
    "\n",
    "        If self.targeted is true, then the targets represents the target labels.\n",
    "        If self.targeted is false, then targets are the original class labels.\n",
    "        \"\"\"\n",
    "        r = []\n",
    "        print('go up to',len(imgs))\n",
    "        for i in range(0,len(imgs),self.batch_size):\n",
    "            print('tick',i)\n",
    "            r.extend(self.attack_batch(imgs[i:i+self.batch_size], targets[i:i+self.batch_size]))\n",
    "        return np.array(r)\n",
    "\n",
    "    def attack_batch(self, imgs, labs):\n",
    "        \"\"\"\n",
    "        Run the attack on a batch of images and labels.\n",
    "        \"\"\"\n",
    "        def compare(x,y):\n",
    "            if not isinstance(x, (float, int, np.int64)):\n",
    "                x = np.copy(x)\n",
    "                if self.TARGETED:\n",
    "                    x[y] -= self.CONFIDENCE\n",
    "                else:\n",
    "                    x[y] += self.CONFIDENCE\n",
    "                x = np.argmax(x)\n",
    "            if self.TARGETED:\n",
    "                return x == y\n",
    "            else:\n",
    "                return x != y\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        # convert to tanh-space\n",
    "        imgs = np.arctanh((imgs - self.boxplus) / self.boxmul * 0.999999)\n",
    "\n",
    "        # set the lower and upper bounds accordingly\n",
    "        lower_bound = np.zeros(batch_size)\n",
    "        CONST = np.ones(batch_size)*self.initial_const\n",
    "        upper_bound = np.ones(batch_size)*1e10\n",
    "\n",
    "        # the best l2, score, and image attack\n",
    "        o_bestl2 = [1e10]*batch_size\n",
    "        o_bestscore = [-1]*batch_size\n",
    "        o_bestattack = [np.zeros(imgs[0].shape)]*batch_size\n",
    "        \n",
    "        for outer_step in range(self.BINARY_SEARCH_STEPS):\n",
    "            print(o_bestl2)\n",
    "            # completely reset adam's internal state.\n",
    "            self.sess.run(self.init)\n",
    "            batch = imgs[:batch_size]\n",
    "            batchlab = labs[:batch_size]\n",
    "    \n",
    "            bestl2 = [1e10]*batch_size\n",
    "            bestscore = [-1]*batch_size\n",
    "\n",
    "            # The last iteration (if we run many steps) repeat the search once.\n",
    "            if self.repeat == True and outer_step == self.BINARY_SEARCH_STEPS-1:\n",
    "                CONST = upper_bound\n",
    "\n",
    "            # set the variables so that we don't have to send them over again\n",
    "            self.sess.run(self.setup, {self.assign_timg: batch,\n",
    "                                       self.assign_tlab: batchlab,\n",
    "                                       self.assign_const: CONST})\n",
    "            \n",
    "            prev = np.inf\n",
    "            for iteration in range(self.MAX_ITERATIONS):\n",
    "                # perform the attack \n",
    "                _, l, l2s, scores, nimg = self.sess.run([self.train, self.loss, \n",
    "                                                         self.l2dist, self.output, \n",
    "                                                         self.newimg])\n",
    "\n",
    "                if np.all(scores>=-.0001) and np.all(scores <= 1.0001):\n",
    "                    if np.allclose(np.sum(scores,axis=1), 1.0, atol=1e-3):\n",
    "                        if not self.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK:\n",
    "                            raise Exception(\"The output of model.predict should return the pre-softmax layer. It looks like you are returning the probability vector (post-softmax). If you are sure you want to do that, set attack.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK = True\")\n",
    "                \n",
    "                # print out the losses every 10%\n",
    "                if iteration%(self.MAX_ITERATIONS//10) == 0:\n",
    "                    print(iteration,self.sess.run((self.loss,self.loss1,self.loss2)))\n",
    "\n",
    "                # check if we should abort search if we're getting nowhere.\n",
    "                if self.ABORT_EARLY and iteration%(self.MAX_ITERATIONS//10) == 0:\n",
    "                    if l > prev*.9999:\n",
    "                        break\n",
    "                    prev = l\n",
    "\n",
    "                # adjust the best result found so far\n",
    "                for e,(l2,sc,ii) in enumerate(zip(l2s,scores,nimg)):\n",
    "                    if l2 < bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        bestl2[e] = l2\n",
    "                        bestscore[e] = np.argmax(sc)\n",
    "                    if l2 < o_bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        o_bestl2[e] = l2\n",
    "                        o_bestscore[e] = np.argmax(sc)\n",
    "                        o_bestattack[e] = ii\n",
    "\n",
    "            # adjust the constant as needed\n",
    "            for e in range(batch_size):\n",
    "                if compare(bestscore[e], np.argmax(batchlab[e])) and bestscore[e] != -1:\n",
    "                    # success, divide const by two\n",
    "                    upper_bound[e] = min(upper_bound[e],CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n",
    "                else:\n",
    "                    # failure, either multiply by 10 if no solution found yet\n",
    "                    #          or do binary search with the known upper bound\n",
    "                    lower_bound[e] = max(lower_bound[e],CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n",
    "                    else:\n",
    "                        CONST[e] *= 10\n",
    "\n",
    "        # return the best solution found\n",
    "        o_bestl2 = np.array(o_bestl2)\n",
    "        return o_bestattack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digit Recognition Model\n",
    "\n",
    "To ensure the run time was effective on my local machine, I halved the training and test samples. My local colab notebook could not handle the large amount of data, even after the reduction to one epoch. With these variables standard, my logical hyper parameter evaluation will still take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup_mnist.py -- mnist data and model loading code\n",
    "##\n",
    "## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n",
    "##\n",
    "## This program is licenced under the BSD 2-Clause licence,\n",
    "## contained in the LICENCE file in this directory.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(num_images*28*28)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = (data / 255) - 0.5\n",
    "        data = data.reshape(num_images, 28, 28, 1)\n",
    "        return data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8)\n",
    "    return (np.arange(10) == labels[:, None]).astype(np.float32)\n",
    "\n",
    "class MNIST:\n",
    "    def __init__(self):\n",
    "        if not os.path.exists(\"data\"):\n",
    "            os.mkdir(\"data\")\n",
    "            files = [\"train-images-idx3-ubyte.gz\",\n",
    "                     \"t10k-images-idx3-ubyte.gz\",\n",
    "                     \"train-labels-idx1-ubyte.gz\",\n",
    "                     \"t10k-labels-idx1-ubyte.gz\"]\n",
    "            for name in files:\n",
    "\n",
    "                urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/' + name, \"data/\"+name)\n",
    "\n",
    "        train_data = extract_data(\"data/train-images-idx3-ubyte.gz\", 30000)\n",
    "        train_labels = extract_labels(\"data/train-labels-idx1-ubyte.gz\", 30000)\n",
    "        self.test_data = extract_data(\"data/t10k-images-idx3-ubyte.gz\", 10000)\n",
    "        self.test_labels = extract_labels(\"data/t10k-labels-idx1-ubyte.gz\", 10000)\n",
    "        \n",
    "        VALIDATION_SIZE = 2500\n",
    "        \n",
    "        self.validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "        self.validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "        self.train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "        self.train_labels = train_labels[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "class MNISTModel:\n",
    "    def __init__(self, restore, session=None):\n",
    "        self.num_channels = 1\n",
    "        self.image_size = 28\n",
    "        self.num_labels = 10\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, (3, 3),\n",
    "                         input_shape=(28, 28, 1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(32, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(10))\n",
    "        model.load_weights(restore)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, data):\n",
    "        return self.model(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural network to prevent the imminent attack. The results will be less accurate, however these will be assessed on an ad hoc basis when compared to the original testing done by the authors. This is because of the number of epochs. I had to reduce the epochs to 1 so that the program could run. However, this will be the only statically reduced variable. All other variables will be assessed with hyperparameter tuning. We will use the results of epoch=1 as the baseline. If other parameters in addition to the epoch change prove more accurate, I will call into question the researcher's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mprint\u001b[39m(predicted)\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(\u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 116\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(\u001b[39m'\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    118\u001b[0m train(MNIST(), \u001b[39m\"\u001b[39m\u001b[39mmodels/mnist\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39m200\u001b[39m], num_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[39m### Reduced epoch to One---Otherwise machine would not tolerate\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'models'"
     ]
    }
   ],
   "source": [
    "## train_models.py -- train the neural network models for attacking\n",
    "##\n",
    "## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n",
    "##\n",
    "## This program is licenced under the BSD 2-Clause licence,\n",
    "## contained in the LICENCE file in this directory.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "\n",
    "\n",
    "### GLENN's changed\n",
    "###Assessing model holistically, assessing model's variables\n",
    "\n",
    "### is structure of the network efficient? Is RELU best activation function\n",
    "\n",
    "def train(data, file_name, params, num_epochs=1, batch_size=128, train_temp=1, init=None):\n",
    "    \"\"\"\n",
    "    Standard neural network training procedure.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    print(data.train_data.shape)\n",
    "    \n",
    "    model.add(Conv2D(params[0], (3, 3),\n",
    "                            input_shape=data.train_data.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(params[1], (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(params[2], (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(params[3], (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(params[4]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(params[5]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(10))\n",
    "    \n",
    "    if init != None:\n",
    "        model.load_weights(init)\n",
    "\n",
    "    def fn(correct, predicted):\n",
    "        return tf.nn.softmax_cross_entropy_with_logits(labels=correct,\n",
    "                                                       logits=predicted/train_temp)\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    model.compile(loss=fn,\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(data.train_data, data.train_labels,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(data.validation_data, data.validation_labels),\n",
    "              epochs=num_epochs,\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    if file_name != None:\n",
    "        model.save(file_name)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "### assessing how the distillation variables work\n",
    "### manipulating training temperature and epochs and batch size\n",
    "def train_distillation(data, file_name, params, num_epochs=1, batch_size=128, train_temp=1):\n",
    "    \"\"\"\n",
    "    Train a network using defensive distillation.\n",
    "\n",
    "    Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks\n",
    "    Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami\n",
    "    IEEE S&P, 2016.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_name+\"_init\"):\n",
    "        # Train for one epoch to get a good starting point.\n",
    "        train(data, file_name+\"_init\", params, 1, batch_size)\n",
    "    \n",
    "    # now train the teacher at the given temperature\n",
    "    teacher = train(data, file_name+\"_teacher\", params, num_epochs, batch_size, train_temp,\n",
    "                    init=file_name+\"_init\")\n",
    "\n",
    "    # evaluate the labels at temperature t\n",
    "    predicted = teacher.predict(data.train_data)\n",
    "    with tf.Session() as sess:\n",
    "        y = sess.run(tf.nn.softmax(predicted/train_temp))\n",
    "        print(y)\n",
    "        data.train_labels = y\n",
    "\n",
    "    # train the student model at temperature t\n",
    "    student = train(data, file_name, params, num_epochs, batch_size, train_temp,\n",
    "                    init=file_name+\"_init\")\n",
    "\n",
    "    # and finally we predict at temperature 1\n",
    "    predicted = student.predict(data.train_data)\n",
    "\n",
    "    print(predicted)\n",
    "    \n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "train(MNIST(), \"models/mnist\", [32, 32, 64, 64, 200, 200], num_epochs=1)\n",
    "\n",
    "\n",
    "\n",
    "### Reduced epoch to One---Otherwise machine would not tolerate\n",
    "train_distillation(MNIST(), \"models/mnist-distilled-100\", [32, 32, 64, 64, 200, 200],\n",
    "                   num_epochs=1, train_temp=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## verify.py -- check the accuracy of a neural network\n",
    "##\n",
    "## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n",
    "##\n",
    "## This program is licenced under the BSD 2-Clause licence,\n",
    "## contained in the LICENCE file in this directory.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    data, model = MNIST(), MNISTModel(\"models/mnist\", sess)\n",
    "\n",
    "    x = tf.placeholder(tf.float32, (None, model.image_size, model.image_size, model.num_channels))\n",
    "    y = model.predict(x)\n",
    "\n",
    "    r = []\n",
    "    for i in range(0,len(data.test_data),BATCH_SIZE):\n",
    "        pred = sess.run(y, {x: data.test_data[i:i+BATCH_SIZE]})\n",
    "        #print(pred)\n",
    "        #print('real',data.test_labels[i],'pred',np.argmax(pred))\n",
    "        r.append(np.argmax(pred,1) == np.argmax(data.test_labels[i:i+BATCH_SIZE],1))\n",
    "        print(np.mean(r))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
